
<div align="center">

# Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling

</div>

<div style="font-family: charter;">
    <a href="#">Gongye Liu*</a>,
    <a href="#">Bo Yang*</a>,
    <a href="#">Zhi Yida</a>,
    <a href="#">Zhizhou Zhong</a>,
    <a href="#">Lei Ke</a>,
    <a href="#">Didan Deng</a>,
    <a href="#">Han Gao</a>,
    <a href="#">Yongxiang Huang</a>,
    <a href="#">Kaihao Zhang</a>,
    <a href="#">Hongbo Fu</a>,
    <a href="#">Wenhan Luo</a>
    <br>
    <sup>1</sup>The Hong Kong University of Science and Technology 
    <sup>2</sup>Huawei Hong Kong AI Framework & Data Technologies Lab  

</div>

> *a diffusion-native latent reward model, competitive reward accuracy, much cheaper for alignment*


<div align="center">
    <img src="assets/method.png" width="800">
</div>


<div align="center">
    <img src="assets/alignment_performance.png" width="800">
</div>

---

## ðŸ§§ Happy Chinese New Year! 

We are actively preparing the repository for public release. The full codebase and pre-trained models will be open-sourced before the **2026 Lunar New Year**. Stay tuned! ðŸš€

---
