import os
import glob
from typing import List, Dict, Any, Optional, Callable, Union
import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, IterableDataset, DataLoader
import io


def npy_bytes_to_ndarray(b: bytes) -> np.ndarray:
    return np.load(io.BytesIO(b), allow_pickle=False)


class SimpleParquetDataset(Dataset):
    def __init__(
        self,
        parquet_files: List[str],
        process_fn: Optional[Callable] = None,
        shuffle: bool = False,
        seed: int = 42,
    ):
        super().__init__()
        
        self.parquet_files = parquet_files
        self.process_fn = process_fn or self._default_process_fn
        self.shuffle = shuffle
        self.seed = seed

        self.data = []

        self.total_samples = 0
        for parquet_file in self.parquet_files:
            df = pd.read_parquet(parquet_file)
            self.total_samples += len(df)

            self.data.append(df)

        self.data = pd.concat(self.data, ignore_index=True)

        if self.shuffle:
            self.data = self.data.sample(frac=1, random_state=self.seed).reset_index(drop=True)

    def _default_process_fn(self, row: Dict) -> Dict:
        """
        é»˜è®¤çš„æ•°æ®å¤„ç†å‡½æ•°ï¼ˆHPDæ ¼å¼ï¼‰
        
        å¯ä»¥é€šè¿‡ä¼ å…¥è‡ªå®šä¹‰process_fnæ¥è¦†ç›–æ­¤å‡½æ•°
        """
        # è§£ç latent
        latent1 = npy_bytes_to_ndarray(row['latent1'])
        latent2 = npy_bytes_to_ndarray(row['latent2'])
        
        # åˆ¤æ–­chosen/reject
        chosen = latent1
        reject = latent2
        
        return {
            'prompt': row['prompt'],
            'latent_chosen': torch.from_numpy(chosen),
            'latent_reject': torch.from_numpy(reject),
        }
    
    def __len__(self):
        """è¿”å›æ•°æ®é›†æ€»æ ·æœ¬æ•°"""
        return self.total_samples

    def __getitem__(self, index: int):
        """æ ¹æ®ç´¢å¼•è¿”å›å•ä¸ªæ ·æœ¬"""
        row = self.data.iloc[index]
        sample = self.process_fn(row)
        return sample


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹å’Œå·¥å…·å‡½æ•°
# ============================================================================

def create_simple_dataloader(
    parquet_files: Union[str, List[str]],
    process_fn: Optional[Callable] = None,
    batch_size: int = 1,
    num_workers: int = 0,
    shuffle: bool = False,
    **kwargs
) -> DataLoader:

    if isinstance(parquet_files, str):
        parquet_files = sorted(glob.glob(parquet_files))

    dataset = SimpleParquetDataset(
        parquet_files=parquet_files,
        process_fn=process_fn,
        shuffle=shuffle,
        **kwargs
    )
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        # collate_fn=collate_fn,
        drop_last=False,
    )




# ============================================================================
# å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

def example_basic_usage():
    """ç¤ºä¾‹1: åŸºæœ¬ä½¿ç”¨"""
    import glob
    
    # è·å–æ‰€æœ‰parquetæ–‡ä»¶
    # parquet_files = glob.glob("/mnt/shangcephfs/mm-base-vision-ascend-2/layke/data/genai-bench/sd3_data/test_strict_aligned_res.parquet")
    parquet_files = glob.glob("/path/to/preprocess_data/sd35m-test/sample_1000.parquet")
    
    loader = create_simple_dataloader(
        parquet_files=parquet_files,
        batch_size=1,
        num_workers=4,
        shuffle=True,
    )
    
    # è¿­ä»£
    for i, sample in enumerate(loader):
        # sample æ˜¯å•ä¸ªæ ·æœ¬çš„dict
        if i % 100 == 0:
            print(f"Sample {i}:")
            print(f"Prompt: {sample['prompt']}")
            print(f"Shape: {sample['latent_chosen'].shape}")
        # import pdb; pdb.set_trace()


def example_custom_process():
    """ç¤ºä¾‹2: è‡ªå®šä¹‰å¤„ç†å‡½æ•°"""
    import glob
    
    def my_process_fn(row):
        """åªåŠ è½½å›¾åƒï¼Œä¸åŒºåˆ†chosen/reject"""
        latent = npy_bytes_to_ndarray(row['latent1'])
        return {
            'image': torch.from_numpy(latent),
            'text': row['prompt'],
        }
    
    dataset = SimpleParquetDataset(
        parquet_files=glob.glob("/path/to/data/*.parquet"),
        process_fn=my_process_fn,  # ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°
    )
    
    loader = DataLoader(dataset, batch_size=1, num_workers=0)
    
    for sample in loader:
        image = sample['image']
        text = sample['text']
        # è®­ç»ƒä»£ç ...


def example_manual_batching():
    """ç¤ºä¾‹3: æ‰‹åŠ¨ç»„batchï¼ˆé€‚åˆéœ€è¦åŠ¨æ€batch_sizeçš„åœºæ™¯ï¼‰"""
    import glob
    
    dataset = SimpleParquetDataset(
        parquet_files=glob.glob("/path/to/data/*.parquet"),
    )
    
    # batch_size=1ï¼Œæ‰‹åŠ¨ç»„batch
    loader = DataLoader(dataset, batch_size=1, num_workers=0)
    
    manual_batch = []
    target_batch_size = 8
    
    for sample in loader:
        manual_batch.append(sample)
        
        if len(manual_batch) >= target_batch_size:
            # ç»„æˆä¸€ä¸ªbatch
            batch = {
                'latent_chosen': torch.stack([s['latent_chosen'] for s in manual_batch]),
                'latent_reject': torch.stack([s['latent_reject'] for s in manual_batch]),
                'prompt': [s['prompt'] for s in manual_batch],
            }
            
            # è®­ç»ƒ
            loss = model(batch)
            loss.backward()
            
            # æ¸…ç©ºbuffer
            manual_batch = []


def example_with_quickstart_function():
    """ç¤ºä¾‹4: ä½¿ç”¨å¿«æ·å‡½æ•°"""
    import glob
    
    # ä¸€è¡Œåˆ›å»ºDataLoader
    loader = create_simple_dataloader(
        parquet_files=glob.glob("/path/to/data/*.parquet"),
        batch_size=1,
        shuffle_files=True,
    )
    
    # ç›´æ¥ä½¿ç”¨
    for sample in loader:
        # ä½ çš„è®­ç»ƒä»£ç 
        pass


# ============================================================================
# æ€§èƒ½æµ‹è¯•
# ============================================================================

def benchmark_reading_speed():
    """æµ‹è¯•è¯»å–é€Ÿåº¦"""
    import time
    import glob
    
    parquet_files = glob.glob("/path/to/data/*.parquet")[:3]  # æµ‹è¯•3ä¸ªæ–‡ä»¶
    
    dataset = SimpleParquetDataset(
        parquet_files=parquet_files,
        verbose=False,
    )
    
    loader = DataLoader(dataset, batch_size=1, num_workers=0)
    
    start_time = time.time()
    sample_count = 0
    
    for sample in loader:
        sample_count += 1
        if sample_count >= 1000:  # åªæµ‹è¯•1000ä¸ªæ ·æœ¬
            break
    
    elapsed = time.time() - start_time
    speed = sample_count / elapsed
    
    print(f"è¯»å–é€Ÿåº¦: {speed:.1f} samples/sec")
    print(f"æ€»æ ·æœ¬æ•°: {sample_count}")
    print(f"æ€»è€—æ—¶: {elapsed:.2f}s")


if __name__ == "__main__":
    print("ğŸš€ SimpleParquetDataset - ç®€å•æµå¼æ•°æ®é›†")
    print("="*80)
    
    # å–æ¶ˆæ³¨é‡Šæ¥è¿è¡Œç¤ºä¾‹
    example_basic_usage()
    # example_custom_process()
    # example_manual_batching()
    # example_with_quickstart_function()
    # benchmark_reading_speed()
    
    print("\nâœ… æ¨¡å—åŠ è½½æˆåŠŸï¼")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("  from simple_parquet_dataset import SimpleParquetDataset")
    print("  from simple_parquet_dataset import create_simple_dataloader  # å¿«æ·å‡½æ•°")